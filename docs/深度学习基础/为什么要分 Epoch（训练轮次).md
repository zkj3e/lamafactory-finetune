# 🔁 为什么要分 Epoch（训练轮次）

## 🧠 一、什么是 Epoch？

**Epoch（轮次）** 表示：
> **整个训练集被模型完整看过一遍的过程。**

在每个 epoch 中，模型会：
1. 遍历训练集中所有样本（通常分成若干个 batch）。
2. 对每个 batch 计算梯度并更新参数。
3. 完成一次“从头到尾”的学习。

---

## ⚙️ 二、与 Batch 的关系

三者关系如下：

| 概念 | 含义 |
|------|------|
| **Epoch** | 整个训练集被训练一次的过程 |
| **Batch Size** | 每次更新参数时使用的样本数量 |
| **Iteration** | 一次参数更新操作 |

它们的关系是：

\[
\text{iterations per epoch} = \frac{\text{样本总数}}{\text{batch size}}
\]

例如：

- 训练集样本数 = 10,000  
- batch size = 100  
- 那么：每个 epoch = 100 次 iteration（更新 100 次参数）  

---

## 🧩 三、为什么要分多个 Epoch？

神经网络不会“一看就会”，需要反复“看多次”数据才能学会规律。  
因此我们要让模型训练多个 epoch，让它逐步收敛。

| 原因 | 说明 |
|------|------|
| 🔄 **反复学习** | 每个 epoch 模型都会在之前的基础上继续优化参数 |
| 🧭 **逐步逼近最优解** | 多次遍历可让损失函数逐渐降低 |
| 🚫 **防止欠拟合** | 如果只训练一次（1 epoch），模型可能还没学到规律 |
| ⚠️ **控制过拟合** | 太多 epoch 可能让模型“死记硬背”训练数据（过拟合） |

---

## 📊 四、训练过程的变化趋势

| Epoch | 训练损失（Train Loss） | 验证损失（Val Loss） | 说明 |
|--------|--------------------------|------------------------|------|
| 1~3 | 快速下降 | 稳定或略降 | 模型在学习主要特征 |
| 4~10 | 缓慢下降 | 稳定 | 模型接近最优点 |
| 10+ | 训练损失继续下降 | 验证损失上升 | 出现过拟合，需早停 |

---

## 🧮 五、举个例子

假设你训练一个模型识别猫狗图片：

- 数据集：10,000 张图片  
- batch size：100  
- epoch：5  

则：
- 每个 epoch 有 100 次参数更新（10,000 ÷ 100 = 100）
- 5 个 epoch 一共更新 500 次参数

训练曲线可能这样变化：

| Epoch | 训练准确率 | 验证准确率 |
|--------|--------------|--------------|
| 1 | 65% | 60% |
| 2 | 78% | 75% |
| 3 | 85% | 83% |
| 4 | 88% | 85% |
| 5 | 90% | 84% （过拟合开始） |

---

## 🎯 六、何时停止训练？

常用策略：
- **Early Stopping（提前停止）**：当验证集 loss 连续几轮不再下降时提前终止训练；
- **学习率调度（LR Scheduler）**：随着 epoch 增加逐步减小学习率；
- **固定轮数**：直接设定 epoch=5、10、20 等。

---

## 🧩 七、形象比喻

> 把训练模型比作学生复习：
>
> - **Batch**：每节课学习一部分知识  
> - **Epoch**：复习整本书一次  
> - **多个 Epoch**：反复复习整本书多次，每次理解更深  
>
> 学太少会忘记（欠拟合），学太多会死记硬背（过拟合）。

---

## 🚀 八、总结

| 项目 | 含义 | 调整影响 |
|------|------|-----------|
| **Epoch** | 训练集被完整训练的次数 | 太少 → 学不够（欠拟合）<br>太多 → 学太过（过拟合） |
| **推荐值** | 一般 3~20（小模型）<br>或直到验证集性能不再提升 | - |
| **结合参数** | 与 batch size、learning rate 一起决定训练效率与结果 | - |

---

> 💡 **一句话总结：**
> - **Batch** 决定每次看多少内容，  
> - **Epoch** 决定一共复习几次整本书。  
> - 训练就是“每次小步走，多轮大循环”。
