# 📦 为什么要分 Batch Size（批次大小）

## 🧠 一、什么是 Batch Size

在训练模型时，我们通常有一个很大的数据集。  
模型通过计算“损失函数的梯度”来更新参数。  
但是——一次性把所有数据都用于计算梯度，**代价太高**！

于是我们把数据分成一批一批的“小份”，每一份称为一个 **batch**。

- **batch size**：每次训练时输入模型的样本数量  
- **epoch**：整个数据集被训练一遍  
- **iteration（迭代）**：一次 batch 的训练和参数更新过程

例如：
> 数据集有 10,000 个样本，batch size=100  
> → 每个 epoch 需要 100 次 iteration（10000 ÷ 100 = 100）

---

## ⚙️ 二、为什么要分 Batch

| 原因 | 说明 |
|------|------|
| 💾 **内存限制** | 数据集太大，无法一次性全部加载到 GPU/CPU 内存中 |
| ⚡ **计算效率** | 一次更新多个样本的平均梯度，比单样本更稳定，收敛更快 |
| 🔁 **梯度估计** | 使用小批量数据的梯度是对全量梯度的近似，可加速训练 |
| 🎯 **泛化能力** | 适度的小 batch 带有“噪声”，有助于跳出局部最优，提高泛化能力 |

---

## 🧮 三、训练方式的三种极端情况

| 类型 | batch size | 特点 | 优缺点 |
|------|-------------|------|--------|
| **全量梯度下降（Full Batch GD）** | = 全部样本 | 每次用整个数据集计算梯度 | ✅ 稳定，理论最优<br>🚫 太慢、内存爆炸 |
| **随机梯度下降（SGD）** | = 1 | 每次用 1 个样本更新 | ✅ 有随机性、能跳出局部最优<br>🚫 不稳定，噪声大 |
| **小批量梯度下降（Mini-Batch GD）** | 一般为 16~4096 | 每次用一小批样本更新 | ✅ 稳定与效率折中，是主流做法 |

---

## 📊 四、Batch Size 对训练的影响

| batch size | 优点 | 缺点 | 适用场景 |
|-------------|------|------|-----------|
| 小（如 16~64） | 泛化能力强、内存占用小 | 训练慢、梯度噪声大 | 小模型、GPU 内存有限 |
| 中（如 128~512） | 平衡训练速度与稳定性 | - | 默认推荐值 |
| 大（如 1024~8192） | 训练速度快、并行高效 | 可能导致收敛不佳、需调大学习率 | 大规模分布式训练（如 GPT、LLaMA） |

> ⚠️ **大 batch 时要适当增大学习率**（Linear Scaling Rule）：
> \[
> \text{new learning rate} = \text{base lr} \times \frac{\text{new batch size}}{\text{base batch size}}
> \]

---

## 🚀 五、直观比喻

> 把训练模型想象成“学习考试”：
>
> - **Batch size = 1**：每道题做完立刻看答案（学得快但容易被噪声干扰）  
> - **Batch size = 全部**：全部做完才看答案（太慢）  
> - **适中 Batch size**：做一部分再总结（高效又稳定）  

---

## 🧩 六、总结

| 项目 | 含义 | 典型值 | 影响 |
|------|------|---------|------|
| **Batch Size** | 每次训练时输入模型的样本数量 | 16~2048 | 影响显存、训练速度、收敛稳定性 |
| **过小** | 学习不稳定，梯度噪声大 | - |
| **过大** | 泛化能力下降、需更多显存 | - |
| **合适** | 稳定收敛、资源高效 | ✅ 推荐使用 |

---

> 💡 **一句话总结：**
> - 分 batch 是为了在计算、内存和模型泛化之间取得平衡。  
> - **小 batch 像勤学苦练，大 batch 像高效冲刺，合适的 batch 才是王道。**
