# 从最简单拟合到深度学习的演变

## 一、从线性拟合开始

### 1. 线性回归
线性回归的核心思想是用一条直线去拟合数据点。

公式：
\[
y = w x + b
\]

特点：
- 能拟合**线性关系**；
- 对复杂曲线无能为力。

---

### 2. 多项式回归
通过引入多项式项（例如 \(x^2, x^3, \dots\)）来模拟非线性关系：

\[
y = w_1 x + w_2 x^2 + w_3 x^3 + b
\]

但：
- 特征是**人工构造的**；
- 特征维度高时容易过拟合；
- 不具备自动学习能力。

---

## 二、单隐层感知机（Single Hidden Layer Perceptron）

### 1. 网络结构

输入层传入数据，隐藏层负责特征提取，输出层做预测。

---

### 2. 工作原理

假设输入：
\[
x = [x_1, x_2, ..., x_n]
\]

#### (1) 隐藏层线性变换：
\[
z_j = \sum_i w_{ij}x_i + b_j
\]

#### (2) 激活函数引入非线性：
\[
h_j = \sigma(z_j)
\]
常见激活函数：ReLU, Sigmoid, Tanh 等。

#### (3) 输出层线性组合：
\[
\hat{y} = \sum_j v_j h_j + c
\]

---

### 3. 本质理解

- 隐藏层：自动学习“特征变换”
- 输出层：对学到的特征做线性分类或回归

即：
> 单隐层感知机 = 自动特征工程 + 线性模型

---

### 4. 为什么能拟合非线性函数

**通用逼近定理（Universal Approximation Theorem）**指出：  
> 只要神经元数量足够，一个单隐层网络可以逼近任意连续函数。

---

## 三、分层结构（深度网络）的意义

### 1. 为什么一层不够
虽然单层网络理论上足够，但：
- 需要非常多的神经元；
- 参数量巨大；
- 拟合复杂函数效率低。

---

### 2. 分层的优势
分层（多层网络）可以**逐层抽象、复用特征**。

- 第一层学：局部特征（如边缘、线条）
- 第二层学：组合特征（如形状、纹理）
- 第三层学：语义特征（如人脸、物体）

每一层都把上一层的输出作为输入，形成**分层表示（Hierarchical Representation）**。

---

### 3. 数学角度：非线性组合的层叠
两层 ReLU 网络：
\[
f(x) = \sum_j a_j \, \text{ReLU}\Big( \sum_i w_{ji} \, \text{ReLU}(u_i x + b_i) + c_j \Big)
\]

- 第二层再对第一层的输出做一次非线性变换；
- “折线段数量”指数增长；
- 表达能力更强。

---

### 4. 直觉比喻

> 一层：所有乐高积木直接搭模型  
> 多层：先搭局部组件，再组合成完整模型  

深度带来更高的组合效率。

---

## 四、为什么前层能学到“局部特征”

### 1. 结构设计决定
例如 CNN：
- 每个卷积核只看局部区域；
- 学到的是边缘、角点等局部模式；
- 后层再组合这些局部特征成更高层抽象。

---

### 2. 训练目标决定
整个网络目标是：
\[
\min_\theta L(f_\theta(x), y)
\]

- 所有层的参数一起优化；
- 若某层输出对降低损失有帮助，它会被强化；
- 层间协同是**自然演化的结果**（通过梯度下降自组织）。

---

## 五、反向传播（Backpropagation）

### 1. 定义
反向传播是一种计算每层参数对整体损失影响的算法。  
即计算：
\[
\frac{\partial L}{\partial W_i}
\]
再用梯度下降更新参数：
\[
W_i \leftarrow W_i - \eta \frac{\partial L}{\partial W_i}
\]

---

### 2. 链式法则：前层也会调整

以两层网络为例：
\[
h = \sigma(W_1 x)
\]
\[
\hat{y} = W_2 h
\]
\[
L = (\hat{y} - y)^2
\]

根据链式法则：
\[
\frac{\partial L}{\partial W_1}
=
\frac{\partial L}{\partial \hat{y}}
\cdot
\frac{\partial \hat{y}}{\partial h}
\cdot
\frac{\partial h}{\partial W_1}
\]

这说明前一层的权重也会被更新。

---

### 3. 直觉理解
反向传播就像反馈链：

每一层都收到“如何调整以减少总体误差”的信号。  
最终所有层都会协同优化。

---

### 4. 没有反向传播会怎样
- 只有最后一层能学习；
- 前层永远不更新；
- 整体学习无效。

反向传播让前后层**共同朝着减少误差的方向努力**。

---

### 5. 实际挑战与改进
在实践中，梯度可能：
- 消失（层数太多，前层收不到信号）
- 爆炸（梯度过大）

解决方法：
| 问题 | 解决方式 |
|------|-----------|
| 梯度消失 | ReLU 激活、良好初始化 |
| 梯度爆炸 | BatchNorm、梯度裁剪 |
| 深层训练难 | 残差连接（ResNet） |

---

## 六、总结

| 阶段 | 关键思想 | 特点 |
|------|------------|------------|
| 线性回归 | 一条直线拟合数据 | 只能表达线性关系 |
| 多项式回归 | 手工增加非线性项 | 容易过拟合 |
| 单隐层感知机 | 自动特征提取 + 非线性 | 可拟合任意连续函数 |
| 多层感知机 | 分层抽象 + 特征复用 | 高效、强大、可泛化 |
| 反向传播 | 全层协同优化 | 每层参数共同更新 |

---

> **一句话总结：**  
> 深度学习的强大在于：  
> 它通过反向传播，让每一层都能自动学习合适的表示，从局部特征到全局语义，逐步逼近真实世界的复杂映射。

