# 🧭 从传统分类到卷积神经网络（CNN）的演变

---

## 一、传统分类方法：人工特征 + 线性分类器

在深度学习出现之前，图像分类主要分为两个步骤：

1. **特征提取（Feature Extraction）**
   - 手工设计特征，如：
     - SIFT（尺度不变特征）
     - HOG（方向梯度直方图）
     - LBP（局部二值模式）
   - 目标：提取图像中“有用的模式”用于区分不同类别。

2. **分类器（Classifier）**
   - 常见方法：
     - 逻辑回归（Logistic Regression）
     - 支持向量机（SVM）
     - 决策树 / 随机森林

这种方法依赖人工经验。  
> 模型学不到“看图”的能力，只能利用预定义特征。

---

## 二、感知机与全连接神经网络（MLP）

### ✳️ 感知机（Perceptron）
最早由 Rosenblatt 在 1958 年提出，用于二分类问题。

- 输入：图像被展开为一维向量
- 输出：二分类结果
- 局限：无法捕捉图像的空间结构

### ✳️ 多层感知机（MLP）
通过堆叠多个隐藏层提升表达能力：

$$
h^{(1)} = \sigma(W^{(1)}x + b^{(1)})
$$
$$
h^{(2)} = \sigma(W^{(2)}h^{(1)} + b^{(2)})
$$
$$
\hat{y} = \text{softmax}(W^{(3)}h^{(2)} + b^{(3)})
$$

虽然能拟合复杂函数，但：
- 参数量极大（图像像素很多）
- 无法识别平移、旋转等局部模式
- 难以泛化

---

## 三、卷积神经网络（CNN）的诞生

> 为了让神经网络更适合“看图”，LeCun 在 1998 年提出了 **LeNet-5** ——  
> 第一代成功的卷积神经网络。

---

## 四、CNN 的核心思想

CNN（Convolutional Neural Network）通过三大机制克服了传统 MLP 的缺点：

### 1️⃣ 卷积层（Convolution Layer）

- 使用局部感受野（local receptive field）提取局部特征：
$$
y_{i,j} = \sum_{m,n} x_{i+m, j+n} \cdot w_{m,n}
$$
- 参数共享：同一个卷积核在整张图片滑动，极大减少参数量。

**直观理解：**
卷积层会自动学习“边缘”、“角点”、“纹理”等局部特征。

---

### 2️⃣ 池化层（Pooling Layer）

- 降低维度，减少计算量；
- 提取主要信息，增强平移不变性；
- 常见方式：最大池化（Max Pooling）

例如：
$$
\text{maxpool}\left(\begin{bmatrix}1 & 3 \\ 2 & 4\end{bmatrix}\right) = 4
$$

---

### 3️⃣ 层叠结构（Stacked Hierarchy）

CNN 通过多层堆叠构建层级特征：

| 层级 | 学到的特征 |
|------|-------------|
| 第1层 | 边缘、线条 |
| 第2层 | 局部形状、纹理 |
| 第3层 | 部分结构（如眼睛、轮子） |
| 高层 | 语义对象（如人脸、动物、汽车） |

> 类似人类视觉系统的分层感知。

---

## 五、经典网络的演化

| 年份 | 模型 | 创新点 |
|------|------|---------|
| 1998 | **LeNet-5** | 手写数字识别，CNN雏形 |
| 2012 | **AlexNet** | GPU训练、ReLU激活、大规模ImageNet成功 |
| 2014 | **VGGNet** | 深层小卷积核结构（3×3） |
| 2015 | **ResNet** | 残差连接解决梯度消失问题 |
| 2016 | **Inception / GoogLeNet** | 多尺度卷积并行 |
| 2019+ | **EfficientNet / ConvNeXt** | 计算与性能兼顾的现代CNN设计 |

---

## 六、CNN 的数学视角

一个典型 CNN 模型结构如下：

$$
\begin{align}
X_0 &= \text{输入图像} \\
X_1 &= \text{Conv}_1(X_0) \\
X_2 &= \text{ReLU}(X_1) \\
X_3 &= \text{Pool}(X_2) \\
X_4 &= \text{Conv}_2(X_3) \\
\hat{y} &= \text{Softmax}(X_4)
\end{align}
$$

每一层都是“线性变换 + 非线性激活”的组合。

---

## 七、CNN 相较传统方法的优势

| 对比项 | 传统分类 | CNN |
|---------|------------|------|
| 特征提取 | 人工定义 | 自动学习 |
| 参数量 | 独立权重 | 参数共享 |
| 空间结构 | 丢失 | 保留局部关系 |
| 平移不变性 | 弱 | 强 |
| 表达能力 | 有限 | 极强 |

---

## 八、深度学习的核心理念延伸

CNN 只是深度学习的一个代表方向。  
后续又衍生出：

| 方向 | 模型 | 应用 |
|------|------|------|
| 序列建模 | RNN / LSTM / GRU | 语音、文本 |
| 自注意力机制 | Transformer | NLP、CV、跨模态 |
| 生成模型 | GAN / VAE | 图像生成、风格迁移 |
| 多模态学习 | CLIP / BLIP / Flamingo | 图文理解、AI对话 |

---

## 九、总结

| 阶段 | 特征方式 | 模型代表 | 核心思想 |
|------|-----------|-----------|------------|
| 人工特征 + 分类器 | 手工提取 + 线性分类 | SIFT + SVM | 人工经验驱动 |
| 单层感知机 | 全连接 | Perceptron | 简单线性分界 |
| 多层感知机 | 全连接 + 激活 | MLP | 非线性映射 |
| 卷积神经网络 | 局部感受野 + 参数共享 | CNN | 自动学习空间特征 |

---

**一句话总结：**  
> CNN 的出现，让计算机第一次“看懂了图像”。  
> 它是从人工规则走向端到端学习的关键转折点。
