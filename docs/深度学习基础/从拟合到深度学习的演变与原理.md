
# 🌊 从拟合到深度学习的演变与原理

---

## 一、从简单拟合开始

最早的机器学习模型，比如 **线性回归** 或 **逻辑回归**，本质上都是在做函数拟合：

$$
y = w_1x_1 + w_2x_2 + ... + b
$$

它只能画出“直线”或“平面”，因此只能拟合 **线性关系**。  
要表达复杂函数，就需要引入 **非线性**。

---

## 二、单隐层感知机（Single Hidden Layer Perceptron）

单隐层感知机是最基础的前馈神经网络，由三层组成：

```
输入层 → 隐藏层 → 输出层
```

### ✳️ 工作流程

1. **线性变换：**
   
$$
z_j = \sum_i w_{ij}x_i + b_j
$$

1. **激活函数（引入非线性）：**
   
$$
h_j = \sigma(z_j)
$$

1. **输出层线性组合：**
   
$$
\hat{y} = \sum_j v_j h_j + c
$$

这样模型就能拟合非线性函数。

---

## 三、通用逼近能力

**通用逼近定理**指出：  
一个具有足够多神经元的单隐层网络，  
可以逼近任何连续函数。

换句话说：

> 单隐层感知机 = 自动特征提取 + 线性分类器。

---

## 四、为什么多层网络更强（分层特征）

虽然单隐层理论上可以拟合任意函数，但在实践中：
- 需要的神经元数量非常多；
- 训练难度高；
- 不容易泛化。

因此，现代神经网络采用 **多层结构（深度学习）**，每一层都学习不同层级的特征：

| 层级 | 典型特征 |
|------|------------|
| 第1层 | 边缘、纹理、基础形状 |
| 第2层 | 局部组合、简单结构 |
| 第3层 | 全局语义、对象概念 |

层层组合就能高效表示复杂模式。

---

## 五、为什么能学到“正确的方向”

这是通过 **反向传播机制（Backpropagation）** 实现的。

### ✳️ 核心思想：

1. **前向传播（Forward Pass）**  
   计算模型输出

   $$ 
    \hat{y} 
   $$

2. **计算损失（Loss）**  
   衡量预测与真实值的差距：

   $$
   L = (\hat{y} - y)^2
   $$

3. **反向传播（Backward Pass）**  
   通过链式法则逐层计算损失对权重的偏导数：

   $$
   \frac{\partial L}{\partial w_i}
   $$

   并更新参数：

   $$
   w_i \leftarrow w_i - \eta \frac{\partial L}{\partial w_i}
   $$

   其中，

   $$
   \eta
   $$  
   是学习率。

4. **结果：**  
   每一层都会 **根据后续层的反馈误差进行调整**。  
   这意味着前面的层会“协同”优化，而不是盲目变坏。

---

## 六、为什么不会完全跑偏？

- **梯度通过链式法则传回前层**，告诉它应该怎样调整才能让最终误差变小；
- **优化算法（如 Adam, SGD）** 控制更新方向与幅度；
- **激活函数与归一化** 防止梯度爆炸或消失。

> 所以虽然每层都是局部调整，但方向是全局误差最小化的“导数方向”，  
> 这保证了整体朝着“正确”的方向前进。

---

## 七、总结

| 阶段 | 主要思想 | 特征 |
|------|------------|--------|
| 简单拟合 | 线性关系 | 只能画直线 |
| 单隐层感知机 | 激活函数引入非线性 | 可逼近任意函数 |
| 多层神经网络 | 分层抽象特征 | 高效表达复杂模式 |
| 反向传播 | 全局误差驱动参数更新 | 自动学习、协同优化 |

---

## 八、直觉理解

- 单层网络就像“人工特征工程”：一次性学习复杂关系；
- 多层网络像“分步理解问题”：先学边缘，再学形状，再学物体；
- 反向传播就像“反馈学习”：每一步的调整都来自最终目标的误差信号。

---

**一句话总结：**
> 深度学习的强大在于：通过反向传播机制，将复杂函数分层表示，并自动学习最优的特征与映射。
